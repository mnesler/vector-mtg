# Component Reusability Matrix for Agentic System

Quick reference for what to reuse vs. build new

---

## Legend
- âœ… **Reuse as-is** - Can be used directly without modification
- ğŸ”„ **Adapt** - Needs minor modifications or wrapper
- ğŸ†• **Build new** - Must be created from scratch
- ğŸ¤” **Evaluate** - Depends on LLM choice and requirements

---

## Data Layer

| Component | Status | Usage in Agent | Notes |
|-----------|--------|----------------|-------|
| PostgreSQL + pgvector | âœ… | Database backend | Production-ready, well-indexed |
| `cards` table | âœ… | Card data source | 509k cards with dual embeddings |
| `rules` table | âœ… | Rule explanations | Pre-classified mechanics |
| `rule_categories` table | âœ… | Category browsing | Hierarchical organization |
| `card_rules` table | âœ… | Card-to-rule mapping | Confidence scores + parameters |
| `rule_interactions` table | âœ… | Combo detection | Synergies, counters, combos |

**New tables needed:**
- ğŸ†• `conversations` - Session management
- ğŸ†• `conversation_messages` - Chat history
- ğŸ†• `conversation_context` - Working memory (deck, filters)

---

## Search & Retrieval

| Component | Status | Usage in Agent | Notes |
|-----------|--------|----------------|-------|
| Hybrid Search Service | ğŸ”„ | Wrap as tool | Query classification + routing |
| Advanced Query Parser | âœ… | Filter extraction | Use for complex queries |
| Embedding Service | âœ… | Semantic search | Keep for card embeddings |
| Rule Engine | ğŸ”„ | Wrap methods as tools | Card lookup, analysis, stats |

**Implementation:**
```python
# Example tool wrapper
def search_cards_tool(query: str, method: str = "auto") -> List[Dict]:
    """Agent tool for searching cards"""
    service = get_hybrid_search_service(db_conn)
    return service.search(query, limit=10)

def get_card_details_tool(card_name: str) -> Dict:
    """Agent tool for card details + rules"""
    engine = MTGRuleEngine(db_conn)
    card = engine.get_card_by_name(card_name)
    rules = engine.get_card_rules(card['id'])
    return {**card, 'rules': rules}
```

---

## LLM & AI

| Component | Status | Usage in Agent | Notes |
|-----------|--------|----------------|-------|
| Phi-3.5 mini (query parser) | ğŸ¤” | Maybe for intent | Too small for conversation |
| all-MiniLM-L6-v2 (embeddings) | âœ… | Keep for cards | Good for semantic search |
| Conversational LLM | ğŸ†• | **Required** | Llama 3.1 70B or GPT-4 |
| Agent framework | ğŸ†• | **Required** | LangChain, LlamaIndex, or custom |
| Tool calling system | ğŸ†• | **Required** | Function calling interface |
| Prompt templates | ğŸ†• | **Required** | System prompts for agent |

**Recommendations:**
- **For Local:** Llama 3.1 70B (needs GPU with 24GB+ VRAM)
- **For Cloud:** GPT-4 Turbo or Claude 3.5 Sonnet
- **For Budget:** GPT-3.5 Turbo (good enough for most tasks)

---

## API & Server

| Component | Status | Usage in Agent | Notes |
|-----------|--------|----------------|-------|
| FastAPI app structure | ğŸ”„ | Keep, add endpoints | Solid foundation |
| REST endpoints | âœ… | Keep for backward compat | `/api/cards/*`, `/api/rules/*` |
| CORS middleware | âœ… | Reuse | Already configured |
| Database lifecycle | âœ… | Reuse | Connection pooling works |
| WebSocket support | ğŸ†• | **Add** | For streaming responses |
| SSE endpoint | ğŸ†• | **Add** | Alternative to WebSocket |

**New endpoints needed:**
- ğŸ†• `POST /api/agent/chat` - Send message, get response
- ğŸ†• `POST /api/agent/session` - Create new session
- ğŸ†• `GET /api/agent/session/{id}` - Get session history
- ğŸ†• `WS /api/agent/stream` - WebSocket for streaming

---

## Conversation Management

| Component | Status | Usage in Agent | Notes |
|-----------|--------|----------------|-------|
| Session tracking | ğŸ†• | **Required** | UUID, created_at, last_active |
| Message persistence | ğŸ†• | **Required** | User + assistant messages |
| Context window | ğŸ†• | **Required** | Last N messages + system prompt |
| Working memory | ğŸ†• | **Required** | Current deck, filters, preferences |
| Long-term memory | ğŸ†• | Optional | User preferences, deck history |

**Architecture:**
```python
class ConversationManager:
    def create_session() -> str
    def get_messages(session_id: str, limit: int = 10) -> List[Dict]
    def add_message(session_id: str, role: str, content: str)
    def get_context(session_id: str) -> Dict
    def update_context(session_id: str, key: str, value: Any)
```

---

## Agent Tools (Functions)

**Card Search Tools:**
- âœ… `search_cards(query)` - Wrapper around hybrid search
- âœ… `get_card_details(name)` - Get full card info + rules
- âœ… `find_similar_cards(name)` - Vector similarity search
- âœ… `filter_cards(filters)` - Advanced filtering

**Card Analysis Tools:**
- âœ… `explain_card_rules(name)` - Get rule explanations
- âœ… `analyze_deck(cards)` - Deck composition analysis
- âœ… `suggest_synergies(card)` - Find synergistic cards
- âœ… `find_combos(cards)` - Detect combo patterns
- ğŸ†• `suggest_cuts(deck)` - Recommend cards to remove

**Deck Building Tools:**
- ğŸ†• `add_to_deck(card, session)` - Add card to working deck
- ğŸ†• `remove_from_deck(card, session)` - Remove card
- ğŸ†• `get_current_deck(session)` - View working deck
- ğŸ†• `optimize_mana_base(deck)` - Suggest lands
- ğŸ†• `check_deck_legality(deck, format)` - Validate deck

**Statistics Tools:**
- âœ… `get_card_count(query)` - How many cards match?
- âœ… `get_rule_stats()` - Overall statistics
- âœ… `get_format_stats(format)` - Format-specific stats

---

## Response Generation

| Component | Status | Usage in Agent | Notes |
|-----------|--------|----------------|-------|
| Natural language generation | ğŸ†• | **Required** | LLM-based |
| Card formatting | ğŸ†• | **Required** | Pretty-print cards |
| Result summarization | ğŸ†• | **Required** | "Found 42 cards, top 5..." |
| Clarification questions | ğŸ†• | **Required** | "What format?" |
| Proactive suggestions | ğŸ†• | Optional | "You might also like..." |
| Error messages | ğŸ†• | **Required** | User-friendly errors |

**Example prompts:**
```python
SYSTEM_PROMPT = """
You are an MTG deck-building assistant with access to a database of 509,000 cards.

Your capabilities:
- Search for cards by name, mechanics, or natural language
- Explain card rules and interactions
- Suggest synergies and combos
- Analyze deck composition
- Help build decks for any format

Guidelines:
- Always confirm format before suggesting cards
- Explain why cards work together
- Ask clarifying questions when query is ambiguous
- Suggest alternatives when original request isn't feasible
- Keep responses concise but informative

Available tools: {tool_list}
"""
```

---

## Testing Strategy

| Component | Status | Notes |
|-----------|--------|-------|
| Existing tests | âœ… | 83% coverage, keep running |
| Tool unit tests | ğŸ†• | Test each tool function |
| Agent integration tests | ğŸ†• | Test multi-turn conversations |
| Prompt testing | ğŸ†• | Validate prompt quality |
| Performance tests | ğŸ†• | Latency, throughput |

**Test scenarios:**
1. Simple search: "Find me red burn spells"
2. Complex filter: "Zombies not black under 3 mana"
3. Multi-turn: Build a deck over 5-10 exchanges
4. Clarification: Handle ambiguous queries
5. Error handling: Invalid card names, impossible filters
6. Context memory: Reference previous results

---

## Deployment Considerations

### Option 1: Local LLM (Llama 3.1 70B)
- âœ… **Pros:** No API costs, full control, privacy
- âŒ **Cons:** Needs GPU (24GB+ VRAM), ~60GB disk, slower
- ğŸ’° **Cost:** One-time hardware ($2000+ for GPU)
- âš¡ **Speed:** 2-5s per response

### Option 2: Cloud LLM (GPT-4 Turbo)
- âœ… **Pros:** Fast, no GPU needed, latest models
- âŒ **Cons:** API costs, latency, less control
- ğŸ’° **Cost:** ~$0.01-0.03 per conversation turn
- âš¡ **Speed:** 1-3s per response

### Option 3: Hybrid Approach
- Use GPT-4 for complex reasoning
- Use local Phi-3.5 for simple tasks (intent classification)
- Use existing embeddings for card search

### Recommended Stack

```yaml
LLM: GPT-4 Turbo (via OpenAI API)
Framework: LangChain (mature, well-documented)
Database: PostgreSQL + pgvector (existing)
API: FastAPI + WebSocket (add to existing)
Frontend: React + TypeScript (existing, add chat UI)

Estimated costs:
- Development: 2 weeks
- Hosting: $50/month (server) + $50-200/month (API calls)
- Performance: 1-3s per response, 95% uptime
```

---

## Implementation Priority

### Phase 1 (Week 1) - Foundation
1. âœ… Add conversation tables to schema
2. âœ… Choose LLM provider (GPT-4 Turbo recommended)
3. âœ… Set up LangChain + OpenAI
4. âœ… Create basic conversation manager
5. âœ… Implement 3-5 core tools

### Phase 2 (Week 2) - Core Agent
1. âœ… Build agent loop with tool calling
2. âœ… Add prompt engineering (system + user prompts)
3. âœ… Implement message persistence
4. âœ… Add context management
5. âœ… Test basic conversations

### Phase 3 (Week 3) - Features
1. âœ… Add all remaining tools
2. âœ… Implement streaming responses
3. âœ… Add working memory (deck builder)
4. âœ… Implement error handling
5. âœ… Add clarification logic

### Phase 4 (Week 4) - Polish
1. âœ… Comprehensive testing
2. âœ… Performance optimization
3. âœ… Add chat UI
4. âœ… Documentation
5. âœ… Production deployment

---

## Quick Start Checklist

### Prerequisites
- [ ] PostgreSQL + pgvector running
- [ ] API server functional
- [ ] 509k cards loaded with embeddings
- [ ] OpenAI API key (or local LLM setup)

### Setup Steps
1. [ ] Install LangChain: `pip install langchain openai`
2. [ ] Add conversation tables (migration script)
3. [ ] Create `ConversationManager` class
4. [ ] Wrap existing methods as tools
5. [ ] Set up LangChain agent
6. [ ] Add `/api/agent/chat` endpoint
7. [ ] Test with simple queries
8. [ ] Add streaming support
9. [ ] Build chat UI
10. [ ] Deploy and monitor

---

**Summary:** ~75% of existing code is reusable. Main work is adding:
1. Conversation management (new)
2. LLM integration (new)
3. Tool wrappers (adapt existing)
4. Streaming responses (new)
5. Chat UI (new)

**Estimated effort:** 2-4 weeks for full implementation.

---

**Document Version:** 1.0  
**Created:** December 2, 2024
